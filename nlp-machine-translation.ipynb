{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11829239,"sourceType":"datasetVersion","datasetId":7403856}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# All necessary imports\n!pip install sacrebleu\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    M2M100ForConditionalGeneration,\n    M2M100Tokenizer,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    EarlyStoppingCallback\n)\nimport sacrebleu\nimport os\n\n# Preprocessing function\ndef preprocess_data(dataset, tokenizer, max_length=128):\n    def tokenize_function(examples):\n        inputs = examples['ur']\n        targets = examples['en']\n        model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length')\n        model_inputs['labels'] = labels['input_ids']\n        return model_inputs\n\n    return dataset.map(tokenize_function, batched=True, remove_columns=['ur', 'en'])\n\n# Evaluation metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels]).score\n    chrf = sacrebleu.corpus_chrf(decoded_preds, [decoded_labels]).score\n    return {\"bleu\": bleu, \"chrf\": chrf}\n\n# Training function\ndef train_on_chunk(file_path, start_idx, chunk_size, num_epochs, resume_from=None):\n    global tokenizer\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"\\n=== Training chunk {start_idx}-{start_idx + chunk_size} ===\")\n\n    # Model setup\n    model_name = \"facebook/m2m100_418M\"\n    output_dir = f\"/kaggle/working/urdu_en_{start_idx}_to_{start_idx + chunk_size}\"\n\n    if resume_from and os.path.exists(resume_from):\n        print(f\"Resuming from {resume_from}\")\n        tokenizer = M2M100Tokenizer.from_pretrained(resume_from)\n        model = M2M100ForConditionalGeneration.from_pretrained(resume_from).to(device)\n    else:\n        print(\"Initializing new model\")\n        tokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang=\"ur\", tgt_lang=\"en\")\n        model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)\n\n    # Load full data\n    df = pd.read_csv(file_path)\n\n    # Slice chunk and filter invalid rows carefully\n    chunk_df = df.iloc[start_idx:start_idx + chunk_size]\n\n    # Drop rows with NaNs in key columns\n    chunk_df = chunk_df.dropna(subset=['Urdu Transalation', 'Sentence'])\n\n    # Filter out empty or non-string rows\n    chunk_df = chunk_df[\n        chunk_df['Urdu Transalation'].apply(lambda x: isinstance(x, str) and x.strip() != \"\") &\n        chunk_df['Sentence'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")\n    ]\n\n    print(f\"Rows in chunk after filtering: {len(chunk_df)}\")\n    if len(chunk_df) == 0:\n        print(f\"No valid data in chunk {start_idx}-{start_idx + chunk_size}, skipping training.\")\n        return resume_from  # Just return last checkpoint path\n\n    # Prepare dataset with renamed columns expected by preprocess_data\n    chunk_df = chunk_df.rename(columns={'Urdu Transalation': 'ur', 'Sentence': 'en'})\n    dataset = Dataset.from_pandas(chunk_df[['ur', 'en']].reset_index(drop=True))\n\n    # Train/val split\n    train_val = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = preprocess_data(train_val[\"train\"], tokenizer)\n    eval_dataset = preprocess_data(train_val[\"test\"], tokenizer)\n\n    # Training arguments with progress tracking\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=2,\n        num_train_epochs=num_epochs,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        fp16=torch.cuda.is_available(),\n        load_best_model_at_end=True,\n        metric_for_best_model=\"bleu\",\n        greater_is_better=True,\n        logging_strategy=\"steps\",\n        logging_steps=50,\n        report_to=\"none\",\n        disable_tqdm=False,\n        log_level=\"info\",\n        predict_with_generate=True,\n        remove_unused_columns=False  # Important to avoid earlier errors\n    )\n\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    print(\"Starting training...\")\n    trainer.train(resume_from_checkpoint=resume_from)\n\n    # Save model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"\\nâœ… Saved checkpoint: {output_dir}\")\n    return output_dir\n\n\n\n# ==== Run training on first chunk ====\nfile_path = \"/kaggle/input/d/bilalnadeem614/nlp-project-dataset/DATA_SET_FOR_NLP_PROJECT.csv\"\nchunk_size = 20000\nresume = None\n\nresume = train_on_chunk(file_path, start_idx=0, chunk_size=chunk_size, num_epochs=3, resume_from=resume)\n\n# Uncomment to run second chunk\n# resume = train_on_chunk(file_path, start_idx=20000, chunk_size=chunk_size, num_epochs=3, resume_from=resume)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T03:50:35.118525Z","iopub.execute_input":"2025-05-17T03:50:35.118816Z","iopub.status.idle":"2025-05-17T05:12:03.971143Z","shell.execute_reply.started":"2025-05-17T03:50:35.118793Z","shell.execute_reply":"2025-05-17T05:12:03.970275Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n\n=== Training chunk 0-20000 ===\nInitializing new model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2d4046750d4d849716b7ea434c400b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b5a7d9c8ea4cd395d8a02256358aa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a1b9dcc7ea4cab96b3117fc06e2cbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b57f616a4b54dc58179663ce4456281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e3822783f5d49b882b1ff976436796f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d59e1cff21d46689143929897b028f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f4245eb5ab4389a34e1a99dde2183e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b8ca36a8794b0aa3175425040985d9"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/447369186.py:59: DtypeWarning: Columns (1,3,8,9,12,13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(file_path)\n","output_type":"stream"},{"name":"stdout","text":"Rows in chunk after filtering: 20000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c886732ef58944358b8bb854d9700f6c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7184c5d20dad46a4b8ff1b8b31cac509"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/447369186.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nUsing auto half precision backend\n***** Running training *****\n  Num examples = 18,000\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 2\n  Total optimization steps = 3,375\n  Number of trainable parameters = 483,905,536\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3375/3375 1:20:52, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Chrf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.096700</td>\n      <td>0.105312</td>\n      <td>43.759828</td>\n      <td>59.255897</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.079000</td>\n      <td>0.097420</td>\n      <td>45.925595</td>\n      <td>60.877150</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.047500</td>\n      <td>0.094969</td>\n      <td>47.177853</td>\n      <td>62.121702</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 8\nGenerate config GenerationConfig {\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 2,\n  \"early_stopping\": true,\n  \"eos_token_id\": 2,\n  \"max_length\": 200,\n  \"num_beams\": 5,\n  \"pad_token_id\": 1\n}\n\nSaving model checkpoint to /kaggle/working/urdu_en_0_to_20000/checkpoint-1125\n/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/config.json\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/generation_config.json\nModel weights saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/model.safetensors\ntokenizer config file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/special_tokens_map.json\nadded tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-1125/added_tokens.json\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 8\nSaving model checkpoint to /kaggle/working/urdu_en_0_to_20000/checkpoint-2250\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/config.json\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/generation_config.json\nModel weights saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/model.safetensors\ntokenizer config file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/special_tokens_map.json\nadded tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-2250/added_tokens.json\n\n***** Running Evaluation *****\n  Num examples = 2000\n  Batch size = 8\nSaving model checkpoint to /kaggle/working/urdu_en_0_to_20000/checkpoint-3375\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/config.json\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/generation_config.json\nModel weights saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/model.safetensors\ntokenizer config file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/special_tokens_map.json\nadded tokens file saved in /kaggle/working/urdu_en_0_to_20000/checkpoint-3375/added_tokens.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /kaggle/working/urdu_en_0_to_20000/checkpoint-3375 (score: 47.17785258025436).\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/config.json\nConfiguration saved in /kaggle/working/urdu_en_0_to_20000/generation_config.json\nModel weights saved in /kaggle/working/urdu_en_0_to_20000/model.safetensors\ntokenizer config file saved in /kaggle/working/urdu_en_0_to_20000/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/urdu_en_0_to_20000/special_tokens_map.json\nadded tokens file saved in /kaggle/working/urdu_en_0_to_20000/added_tokens.json\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Saved checkpoint: /kaggle/working/urdu_en_0_to_20000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import shutil\nimport os\n\nmodel_dir = \"/kaggle/working/urdu_en_0_to_20000/\"\n\n# Delete all checkpoint folders\nfor sub in os.listdir(model_dir):\n    if sub.startswith(\"checkpoint-\"):\n        shutil.rmtree(os.path.join(model_dir, sub))\n\n# Delete TrainerState and training arguments (optional)\nfor fname in [\"trainer_state.json\", \"training_args.bin\", \"all_results.json\"]:\n    fpath = os.path.join(model_dir, fname)\n    if os.path.exists(fpath):\n        os.remove(fpath)\n\nprint(\"âœ… Cleaned up unnecessary files.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T05:12:25.827491Z","iopub.execute_input":"2025-05-17T05:12:25.827796Z","iopub.status.idle":"2025-05-17T05:12:27.547545Z","shell.execute_reply.started":"2025-05-17T05:12:25.827772Z","shell.execute_reply":"2025-05-17T05:12:27.546644Z"}},"outputs":[{"name":"stdout","text":"âœ… Cleaned up unnecessary files.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!zip -r /kaggle/working/urdu_en_0_to_20000.zip /kaggle/working/urdu_en_0_to_20000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T05:17:18.635179Z","iopub.execute_input":"2025-05-17T05:17:18.635896Z","iopub.status.idle":"2025-05-17T05:18:58.346202Z","shell.execute_reply.started":"2025-05-17T05:17:18.635866Z","shell.execute_reply":"2025-05-17T05:18:58.345307Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/urdu_en_0_to_20000/ (stored 0%)\n  adding: kaggle/working/urdu_en_0_to_20000/sentencepiece.bpe.model (deflated 50%)\n  adding: kaggle/working/urdu_en_0_to_20000/generation_config.json (deflated 35%)\n  adding: kaggle/working/urdu_en_0_to_20000/config.json (deflated 57%)\n  adding: kaggle/working/urdu_en_0_to_20000/special_tokens_map.json (deflated 79%)\n  adding: kaggle/working/urdu_en_0_to_20000/model.safetensors (deflated 7%)\n  adding: kaggle/working/urdu_en_0_to_20000/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/urdu_en_0_to_20000/added_tokens.json (deflated 76%)\n  adding: kaggle/working/urdu_en_0_to_20000/vocab.json (deflated 71%)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\nmodel_dir = \"/kaggle/working/urdu_en_0_to_20000/\"\nfor fname in os.listdir(model_dir):\n    full_path = os.path.join(model_dir, fname)\n    if os.path.isfile(full_path):\n        display(FileLink(full_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T04:43:48.932499Z","iopub.execute_input":"2025-05-16T04:43:48.932798Z","iopub.status.idle":"2025-05-16T04:43:48.946475Z","shell.execute_reply.started":"2025-05-16T04:43:48.932769Z","shell.execute_reply":"2025-05-16T04:43:48.945855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset_metadata = \"\"\"\n# {\n#   \"title\": \"urdu_en_0_to_20000_model\",\n#   \"id\": \"your-kaggle-username/urdu-en-0-to-20000-model\",\n#   \"licenses\": [\n#     {\n#       \"name\": \"CC0-1.0\"\n#     }\n#   ]\n# }\n# \"\"\"\n\n# with open('/kaggle/working/dataset-metadata.json', 'w') as f:\n#     f.write(dataset_metadata)\n\n!kaggle datasets create \\\n  -p /kaggle/working \\\n  -u \\\n  -r zip \\\n  -m /kaggle/working/dataset-metadata.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T05:24:59.454551Z","iopub.execute_input":"2025-05-17T05:24:59.455087Z","iopub.status.idle":"2025-05-17T05:25:00.237478Z","shell.execute_reply.started":"2025-05-17T05:24:59.455064Z","shell.execute_reply":"2025-05-17T05:25:00.236673Z"}},"outputs":[{"name":"stdout","text":"Traceback (most recent call last):\n  File \"/usr/local/bin/kaggle\", line 4, in <module>\n    from kaggle.cli import main\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/__init__.py\", line 6, in <module>\n    api.authenticate()\n  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 433, in authenticate\n    raise IOError('Could not find {}. Make sure it\\'s located in'\nOSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!rm -rf /kaggle/working/urdu_en_0_to_20000.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T04:46:04.027673Z","iopub.execute_input":"2025-05-16T04:46:04.027974Z","iopub.status.idle":"2025-05-16T04:46:04.196341Z","shell.execute_reply.started":"2025-05-16T04:46:04.027947Z","shell.execute_reply":"2025-05-16T04:46:04.195306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Load model and tokenizer\nmodel_dir = \"/kaggle/working/urdu_en_0_to_20000\"  # Adjust if saved elsewhere\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Urdu test sentences\nurdu_sentences = [\n    \"Ø¢Ù¾ Ú©ÛŒØ³Û’ ÛÛŒÚºØŸ\",\n    \"Ù…Ø¬Ú¾Û’ Ø¢Ø¬ Ø¨ÛØª Ú©Ø§Ù… Ú©Ø±Ù†Ø§ ÛÛ’Û”\",\n    \"Ú©ÛŒØ§ ØªÙ… Ù…ÛŒØ±ÛŒ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛ’ ÛÙˆØŸ\",\n    \"Ø¢Ø¬ Ú©Ø§ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”\",\n    \"Ù…ÛŒÚº Ú©ØªØ§Ø¨ Ù¾Ú‘Ú¾ Ø±ÛØ§ ÛÙˆÚºÛ”\"\n]\n\n# Translation loop\nfor sentence in urdu_sentences:\n    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    translated_tokens = model.generate(\n        **inputs,\n        max_length=200,\n        num_beams=5,\n        early_stopping=True\n    )\n    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n    print(f\"ğŸ”¸ Urdu: {sentence}\\nğŸ”¹ English: {translated_text}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T04:50:10.779412Z","iopub.execute_input":"2025-05-16T04:50:10.780093Z","iopub.status.idle":"2025-05-16T04:50:13.413935Z","shell.execute_reply.started":"2025-05-16T04:50:10.780066Z","shell.execute_reply":"2025-05-16T04:50:13.413067Z"}},"outputs":[],"execution_count":null}]}